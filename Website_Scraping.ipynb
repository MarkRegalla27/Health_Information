{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Assumed unreliable sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://www.homeoint.org    #Scrapes Homeopathy site\n",
    "for i in range(40):\n",
    "    url = 'http://www.homeoint.org/hompath/articles/' + str(i) + '.html'\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 404:\n",
    "        try:\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            titles = soup.find_all('title')\n",
    "            the_title = titles[0].text\n",
    "            text = soup.find_all('br')\n",
    "            content = text[0].text\n",
    "            with open('/users/markregalla/desktop/metis/Project4/HomeoArticles/page_' \n",
    "                      + str(i) +'.txt', 'w') as myfile:\n",
    "                #myfile.write((the_title + '\\n' + content).encode(\"UTF-8\"))\n",
    "                myfile.write((the_title + content).encode(\"UTF-8\"))\n",
    "            myfile.close\n",
    "        except:\n",
    "            pass\n",
    "    sys.stdout.write(\"\\r\" + str(i))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://www.homeopathycenter.org/   #doesn't take many requests to jam up server\n",
    "url = 'http://www.homeopathycenter.org/find-remedy/a'# + letters[0]\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "text = soup.find_all('p')\n",
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters = [string.ascii_lowercase]\n",
    "letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Assumed reliable sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mayo Clinic Drugs and supplements articles\n",
    "url = 'http://www.mayoclinic.org/drugs-supplements'\n",
    "r = requests.get(url)\n",
    "print r.status_code\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "i = 0\n",
    "ref_list = []\n",
    "for link in soup.find_all('a'):\n",
    "    if i in range(108, 149):\n",
    "        ref_list.append(link['href'])\n",
    "    i += 1\n",
    "k = 0\n",
    "for link in ref_list:\n",
    "    r = requests.get('http://www.mayoclinic.org/' + link)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    texts = soup.find_all('p')\n",
    "    text_list = []\n",
    "    i = 0 \n",
    "    for text in texts:\n",
    "        text_list.append(text.text)\n",
    "        i += 1\n",
    "    \n",
    "    document = ''\n",
    "    for element in text_list[3:len(texts) - 8]:\n",
    "        document = document + str(element.encode(\"UTF-8\"))\n",
    "    try:\n",
    "        with open('/users/markregalla/desktop/metis/Project4/MayoArticles/page_' \n",
    "                          + str(k) +'.txt', 'w') as myfile:\n",
    "            myfile.write(document.encode(\"UTF-8\"))\n",
    "        myfile.close\n",
    "    except:\n",
    "        pass\n",
    "    sys.stdout.write(\"\\r\" + str(k)) \n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Experimental total scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function takes website request text and scrapes all links on that site\n",
    "#Not functioning properly yet\n",
    "def Scrape_Whole_Site(url):\n",
    "    link_list = []\n",
    "    master_list = []\n",
    "    successes = 0\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    master_links = soup.find_all('a')\n",
    "    \n",
    "    for link in master_links:\n",
    "        link_list.append(link['href'])\n",
    "        master_list.append(link['href'])\n",
    "    \n",
    "    #for link in link_list:\n",
    "        #print link\n",
    "        #print link[:len(url)]\n",
    "    \n",
    "    for link in link_list:\n",
    "        try:\n",
    "            sub_request = requests.get(link)\n",
    "        except:\n",
    "            pass\n",
    "        sub_soup = BeautifulSoup(sub_request.text, 'html.parser')\n",
    "        sub_links = sub_soup.find_all('a')\n",
    "        for sub_link in sub_links:\n",
    "            try:\n",
    "                if sub_link['href'][:len(url)] == url:\n",
    "                    if sub_link['href'] not in master_list:\n",
    "                        #print sub_link['href']\n",
    "                        master_list.append(sub_link['href'])\n",
    "                        successes += 1\n",
    "                        sys.stdout.write(\"\\r\" + 'Sucesses: ' + str(successes))   \n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    first_len = len(master_list)\n",
    "    set_list = set(master_list)\n",
    "    set_len = len(master_list)\n",
    "    \n",
    "    return first_len, set_len\n",
    "    \n",
    "first_len, set_len = Scrape_Whole_Site(url)\n",
    "print ' '\n",
    "print 'Length: ' + str(first_len)\n",
    "print 'Set len: ' + str(set_len)\n",
    "print 'Done'\n",
    "for url in master_list:\n",
    "    print url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Wikipedia scraping for Doc2Vec training\n",
    "Runs them through training model\n",
    "\n",
    "Wikipedia API guide:\n",
    "https://en.wikipedia.org/wiki/Special:ApiSandbox#action=query&prop=info&format=json&inprop=url&pageids=1000005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "i = 259987          #i = 259987   (12-5-15 1:43 pm)\n",
    "counter = 50000   #counter = 50000 (12-5-15 1:43 pm)\n",
    "good_id_list = [] #be sure only to initialize as empty list the first time\n",
    "#id_list = []      #use new list if you want to preserve good_id_list in memory\n",
    "\n",
    "while counter <= 70000:   #number is limit of number of page ids to get\n",
    "    scrape_worked = False\n",
    "    #if i == 1000:\n",
    "    #    print 'went on too long'\n",
    "    #    break\n",
    "    page_id = 1000000 + i\n",
    "    \n",
    "    #try to scrape webpage.  Ignore if bad page error is returned from wikipedia\n",
    "    try:\n",
    "        r = requests.get('https://en.wikipedia.org/w/api.php?action=query&prop=info\\\n",
    "                &format=json&inprop=url&pageids=' + str(page_id))\n",
    "        rtext = r.text\n",
    "        jsontext = json.loads(rtext)\n",
    "        if jsontext['query']['pages'][str(page_id)]['ns'] == 0:\n",
    "            scrape_worked = True\n",
    "    except:\n",
    "        i += 1    \n",
    "    \n",
    "    if scrape_worked == True:\n",
    "        article = requests.get('https://en.wikipedia.org/?curid=' + str(page_id))\n",
    "        soup = BeautifulSoup(article.text, 'html.parser')\n",
    "        paragraphs = soup.findAll('p')\n",
    "        if len(paragraphs) >= 10:\n",
    "            good_id_list.append(page_id)\n",
    "            #id_list.append(page_id)    #for preservation of good_id_list\n",
    "            corpus = page_read(soup)\n",
    "            corpus = [corpus]\n",
    "            corpus = cleanText(corpus)\n",
    "            #write each wikipedia article to its own file\n",
    "            with open(\"/users/markregalla/desktop/metis/Project4/WikiArticles/page_\" \n",
    "                      + str(page_id) + \".txt\", \"w\") as myfile:\n",
    "                myfile.write(corpus.encode(\"UTF-8\"))\n",
    "            myfile.close\n",
    "            counter += 1\n",
    "    i += 1\n",
    "    \n",
    "    #prints iterators as a status\n",
    "    sys.stdout.write(\"\\r\" + 'Attempts: ' + str(i) + '  Sucesses: ' + str(counter))   \n",
    "\n",
    "print '\\n' + str(len(good_id_list))\n",
    "#print '\\n' + str(len(id_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
